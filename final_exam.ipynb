{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나랏말', '이', '중국', '과', '달라', '한자', '와', '서로', '통', '하지', '아니하므로', '우매', '한', '백성', '들', '이', '말', '하고', '싶은', '것', '이', '있어도', '마침내', '제', '뜻', '을', '잘', '표현', '하지', '못', '하는', '사람', '이', '많다', '.', '내', '이를', '딱하게', '여기어', '새로', '스물', '여덟', '자를', '만들었으니', '사람', '들', '로', '하여금', '쉬', '익히어', '날', '마다', '쓰는', '데', '편하게', '할', '뿐', '이다']\n",
      "58\n"
     ]
    }
   ],
   "source": [
    "\"\"\"문제1. Tokenization\n",
    "\n",
    "1-1. Okt를 이용하여 'sample_corpus_hangul.txt' corpus를 불러와 형태 단위로 토큰화 하기\n",
    "1-2. 토큰 전체 개수 구하기\n",
    "\n",
    "Hint:\n",
    "- 불필요한 공백 및 개행문자 제거: strip()\n",
    "- list, set, dictinary, 문자열 개수 구하기: len()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "with open('sample_corpus_hangul.txt', encoding='utf-8') as f:\n",
    "    corpus = [line.strip() for line in f.readlines()]\n",
    "    corpus = ' '.join(corpus)\n",
    "\n",
    "okt = Okt()\n",
    "tokens = okt.morphs(corpus)\n",
    "print(tokens)\n",
    "print(len(tokens))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>문의입니다</th>\n",
       "      <th>버스</th>\n",
       "      <th>승강장</th>\n",
       "      <th>지하철</th>\n",
       "      <th>코로나</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.326550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.945180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.285703</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.958318</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.388500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.651563</td>\n",
       "      <td>0.651563</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.568471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.822704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.512131</td>\n",
       "      <td>0.858908</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.512131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.858908</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      문의입니다        버스       승강장       지하철       코로나\n",
       "0  0.326550  0.000000  0.000000  0.000000  0.945180\n",
       "1  0.285703  0.000000  0.000000  0.958318  0.000000\n",
       "2  0.388500  0.000000  0.651563  0.651563  0.000000\n",
       "3  0.568471  0.000000  0.000000  0.000000  0.822704\n",
       "4  0.512131  0.858908  0.000000  0.000000  0.000000\n",
       "5  0.000000  1.000000  0.000000  0.000000  0.000000\n",
       "6  0.000000  0.000000  0.000000  0.000000  1.000000\n",
       "7  0.512131  0.000000  0.858908  0.000000  0.000000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"문제2. TF_IDF\n",
    "'sample_corpus.txt' corpus를 line by line으로 불러와, TF_IDF text mining 하기\n",
    "text mining 결과는 pandas DataFrame 으로 생성  \n",
    "- 분석단위: 'word'\n",
    "- 토큰 최대 개수: 5\n",
    "\n",
    "Hint:\n",
    "- from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "- import pandas as pd\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "with open('sample_corpus.txt', encoding='utf-8') as f:\n",
    "    corpus = [line.strip() for line in f.readlines()]\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word',\n",
    "                             max_features=5)\n",
    "\n",
    "dtm = tfidf_vect.fit_transform(corpus)\n",
    "vocab = tfidf_vect.get_feature_names_out()\n",
    "df_tfidf = pd.DataFrame(dtm.toarray(), columns=vocab)\n",
    "df_tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>149900</th>\n",
       "      <td>9392240</td>\n",
       "      <td>진짜엄청재미없다 무서운장면은 딱한번나오고 그3초말고는 하나도안무섭고 지루하고 끝나고...</td>\n",
       "      <td>0</td>\n",
       "      <td>[진짜, 장면, 한번, 그, 하나, 귀신, 햇던]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149901</th>\n",
       "      <td>10141935</td>\n",
       "      <td>너무 짜증나서, 끝은봐야해서 2배속빨리보기했다. 안본사람 호기심이라도 같지말길. 가...</td>\n",
       "      <td>0</td>\n",
       "      <td>[끝, 배속, 보기, 안, 사람, 호기심, 가치, 강비, 추]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149902</th>\n",
       "      <td>9641183</td>\n",
       "      <td>설명은 줄이고 조율이 필요한 영화.</td>\n",
       "      <td>0</td>\n",
       "      <td>[설명, 줄, 조율, 영화]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149903</th>\n",
       "      <td>10276386</td>\n",
       "      <td>이야~~~40만 넘었내..베테랑은 9백만 넘었는데...나 협심증 걸렸다</td>\n",
       "      <td>0</td>\n",
       "      <td>[베테, 백만, 나, 협심증]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149904</th>\n",
       "      <td>4403898</td>\n",
       "      <td>전형적인 유치뽕짝 프랑스영화 새로운장르다 유치뽕짝</td>\n",
       "      <td>0</td>\n",
       "      <td>[전형, 유치, 뽕짝, 프랑스, 영화, 장르, 유치, 뽕짝]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149995</th>\n",
       "      <td>6222902</td>\n",
       "      <td>인간이 문제지.. 소는 뭔죄인가..</td>\n",
       "      <td>0</td>\n",
       "      <td>[인간, 문제, 소, 죄인]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149996</th>\n",
       "      <td>8549745</td>\n",
       "      <td>평점이 너무 낮아서...</td>\n",
       "      <td>1</td>\n",
       "      <td>[평점]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149997</th>\n",
       "      <td>9311800</td>\n",
       "      <td>이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?</td>\n",
       "      <td>0</td>\n",
       "      <td>[이, 뭐, 한국인, 먹거리, 필리핀, 혼혈]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149998</th>\n",
       "      <td>2376369</td>\n",
       "      <td>청춘 영화의 최고봉.방황과 우울했던 날들의 자화상</td>\n",
       "      <td>1</td>\n",
       "      <td>[청춘, 영화, 최고봉, 방황, 날, 자화상]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149999</th>\n",
       "      <td>9619869</td>\n",
       "      <td>한국 영화 최초로 수간하는 내용이 담긴 영화</td>\n",
       "      <td>0</td>\n",
       "      <td>[한국, 영화, 최초, 수간, 내용, 영화]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  ...                               token\n",
       "149900   9392240  ...         [진짜, 장면, 한번, 그, 하나, 귀신, 햇던]\n",
       "149901  10141935  ...  [끝, 배속, 보기, 안, 사람, 호기심, 가치, 강비, 추]\n",
       "149902   9641183  ...                     [설명, 줄, 조율, 영화]\n",
       "149903  10276386  ...                    [베테, 백만, 나, 협심증]\n",
       "149904   4403898  ...   [전형, 유치, 뽕짝, 프랑스, 영화, 장르, 유치, 뽕짝]\n",
       "...          ...  ...                                 ...\n",
       "149995   6222902  ...                     [인간, 문제, 소, 죄인]\n",
       "149996   8549745  ...                                [평점]\n",
       "149997   9311800  ...           [이, 뭐, 한국인, 먹거리, 필리핀, 혼혈]\n",
       "149998   2376369  ...           [청춘, 영화, 최고봉, 방황, 날, 자화상]\n",
       "149999   9619869  ...            [한국, 영화, 최초, 수간, 내용, 영화]\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"문제3. Tokenization and DataFrame\n",
    "\n",
    "Tab 으로 구분된 '영화감상평' 텍스트 파일 'ratings_train.txt'를 pandas DataFrame으로 불러와 토큰화하기\n",
    "- 마지막 100개 row만 사용\n",
    "- 토큰화 객체는 Okt를 사용\n",
    "- 토큰은 명사(Noun)만 추출\n",
    "- 토큰 결과는 DataFrame 에 'embedding' column에 생성할 것\n",
    "\n",
    "Hint:\n",
    "- Okt.pos()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_table('ratings_train.txt')[-100:]\n",
    "\n",
    "def tokenizer(okt: Okt, text:str)->list:\n",
    "    return [token for token, pos in okt.pos(text) if pos=='Noun']\n",
    "    \n",
    "df['token'] = df['document'].apply(lambda x: tokenizer(okt, x))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'triu' from 'scipy.linalg' (c:\\Users\\IN4U\\kclass\\lib\\site-packages\\scipy\\linalg\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 15\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"문제4. embedding\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m문제3 에서 생성된 token을 이용하여 embedding 생성하기\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m- pandas DataFrame에 'embedding' column 을 embedding 값 생성\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2vec\n",
      "File \u001b[1;32mc:\\Users\\IN4U\\kclass\\lib\\site-packages\\gensim\\__init__.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\IN4U\\kclass\\lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\IN4U\\kclass\\lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[0;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[1;32mc:\\Users\\IN4U\\kclass\\lib\\site-packages\\gensim\\interfaces.py:19\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[0;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[1;32mc:\\Users\\IN4U\\kclass\\lib\\site-packages\\gensim\\matutils.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m entropy\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_blas_funcs, triu\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlapack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m psi  \u001b[38;5;66;03m# gamma function utils\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'triu' from 'scipy.linalg' (c:\\Users\\IN4U\\kclass\\lib\\site-packages\\scipy\\linalg\\__init__.py)"
     ]
    }
   ],
   "source": [
    "\"\"\"문제4. embedding\n",
    "문제3 에서 생성된 token을 이용하여 embedding 생성하기\n",
    "- pandas DataFrame에 'embedding' column 을 embedding 값 생성\n",
    "- Word2Vec 객체 사용\n",
    "  벡터 크기: 50\n",
    "  최소 빈도수: 2\n",
    "  윈도우 5\n",
    "  skip gram : 사용\n",
    "\n",
    "Hint:\n",
    "- from gensim.models import Word2Vec\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from gensim.models import Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
