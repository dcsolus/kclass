{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 간단한 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'down', 'a', 'text', 'into', 'smaller', 'units', 'called', 'tokens.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##### 간단한 토큰화 ####\n",
    "text = \"Tokenization is the process of breaking down a text into smaller units called tokens.\"\n",
    "\n",
    "tokens = text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 코퍼스 불러와서 간단한 토큰 연습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나랏말이중국과 달라 한자와 서로 통하지 아니하므로', '우매한 백성들이 말하고 싶은 것이 있어도 마침내 제 뜻을 잘 표현하지 못하는 사람이 많다.', '내 이를 딱하게 여기어 새로 스물여덟 자를 만들었으니', '사람들로 하여금 쉬 익히어 날마다 쓰는 데 편하게 할 뿐이다']\n"
     ]
    }
   ],
   "source": [
    "with open('sample_corpus_hangul.txt', encoding='utf-8') as f:\n",
    "    corpus = f.readlines()\n",
    "    corpus = [line.strip() for line in corpus] \n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나랏말이중국과', '달라', '한자와', '서로', '통하지', '아니하므로']\n",
      "['우매한', '백성들이', '말하고', '싶은', '것이', '있어도', '마침내', '제', '뜻을', '잘', '표현하지', '못하는', '사람이', '많다.']\n",
      "['내', '이를', '딱하게', '여기어', '새로', '스물여덟', '자를', '만들었으니']\n",
      "['사람들로', '하여금', '쉬', '익히어', '날마다', '쓰는', '데', '편하게', '할', '뿐이다']\n"
     ]
    }
   ],
   "source": [
    "for line in corpus:\n",
    "    tokens = line.split()\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모듈을 이용한 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'down', 'a', 'text', 'into', 'smaller', 'units', 'called', 'tokens', '.']\n",
      "['나랏말이중국과', '달라', '한자와', '서로', '통하지', '아니하므로']\n",
      "['우매한', '백성들이', '말하고', '싶은', '것이', '있어도', '마침내', '제', '뜻을', '잘', '표현하지', '못하는', '사람이', '많다', '.']\n",
      "['내', '이를', '딱하게', '여기어', '새로', '스물여덟', '자를', '만들었으니']\n",
      "['사람들로', '하여금', '쉬', '익히어', '날마다', '쓰는', '데', '편하게', '할', '뿐이다']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##### 모듈을 이용한 토큰화 ############\n",
    "# pip install nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Word tokenization using NLTK\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Print the tokens\n",
    "print(tokens)\n",
    "\n",
    "for line in corpus:\n",
    "    tokens = word_tokenize(line) \n",
    "    print(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
